# USA Capital City Wikipedia Crawler

Crawler reads through table containing US captials and collects given information from each city. All data is added to a Pandas Dataframe and exported to a CSV file for distribution.

This program was created to teach students how to create a crawler. First, they learn how to scrape a table from a given page and add informaton to a dataframe, then they advance to a crawler accessing each capital city in America listed on Wikipedia. There are comments in the code to help describe what is being asked of them and how to do it.

Since this was developed as a tutorial of the Portland State University, students are then asked to clean the data in any way they seem fit and was not done in this code.

### Included in Repo

- Crawler python program
- Jupyter Notebook for scraper tutorial
- Jupyter Notebook for crawler tutorial
- Example csv of collected uncleaned data

### How to Run

```python3 crawler.py```
